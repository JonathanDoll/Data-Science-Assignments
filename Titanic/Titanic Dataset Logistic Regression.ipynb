{
 "metadata": {
  "name": "",
  "signature": "sha256:1632107776527165a3069c570b60d52a007f98a1a90f762c00b8bbdeef2559ff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Logistic Regression for the Titanic Dataset\n",
      "When working with this dataset, I decided to alter a few of the variables to create my own.  I'll explain what and why as the steps get to it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pnd\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseData = pnd.read_csv(\"train.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to populate the missing values for the age, I decided to just go with the mean.  Granted there may have been a better/complicated options, I decided to use my WC variable created below to help \"pick up the slack\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseData.Age = baseData.Age.fillna(value=baseData.Age.mean())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To help reduce the amount of variables, I decided to combine Parch and SibSp into one variable, Rel (relative). Although it didn't affect the accuracy any (for using Parch + SibSp versus Rel), I figured I'd just leave it in.\n",
      "\n",
      "Master was a common name used to describe children. Since I didn't have a definative age for what would count for a child, I figured it would be better just to have a binary variable for using what was considered a child back then (a Master). I felt this method would add more weight to saying \"Hey, this is actually a child\" when encountering someone using the average age. This would also go into seeing if \"Women and Children\" had a better survival rate.  Using this method and in a sense treating children as if they were women, I was able to eliminate the \"Sex\" variable.\n",
      "\n",
      "Even with the WC variable, age was still left in for things such as older and elderly people.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(baseData.Name)):\n",
      "    if \"Master.\" in baseData.Name[i] or \"female\" in baseData.Sex[i]:\n",
      "        baseData.Name[i] = 1\n",
      "    else:\n",
      "        baseData.Name[i] = 0\n",
      "    baseData.Parch[i] = baseData.Parch[i] + baseData.SibSp[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pnd.DataFrame()\n",
      "data[\"Survived\"] = baseData[\"Survived\"]\n",
      "data[\"Pclass\"] = baseData[\"Pclass\"]\n",
      "data[\"Age\"] = baseData[\"Age\"]\n",
      "data[\"WC\"] = baseData[\"Name\"]\n",
      "data[\"Rel\"] = baseData[\"Parch\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = data[\"Survived\"]\n",
      "data = data.drop([\"Survived\"], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "scaler = StandardScaler()\n",
      "data = scaler.fit_transform(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_train, data_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression(penalty=\"l1\", C=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.fit(data_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "prev = accuracy_score(y_test, model.predict(data_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cVal = 100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "while cVal > 0:\n",
      "    cVal -= 0.05\n",
      "    model = LogisticRegression(penalty=\"l1\", C=cVal)\n",
      "    model.fit(data_train, y_train)\n",
      "    cur = accuracy_score(y_test, model.predict(data_test))\n",
      "    if cur < prev:\n",
      "        cVal += 0.05\n",
      "        break\n",
      "    prev = cur"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The highest accuracy score achieved from altering the C value is shown below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LogisticRegression(penalty=\"l1\", C=cVal)\n",
      "model.fit(data_train, y_train)\n",
      "accuracy_score(y_test, model.predict(data_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "0.82681564245810057"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.metrics import classification_report"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "roc_auc_score(y_test, model.predict(data_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "0.81647361647361649"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classification_report(y_test, model.predict(data_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.84      0.88      0.86       105\n",
        "          1       0.81      0.76      0.78        74\n",
        "\n",
        "avg / total       0.83      0.83      0.83       179\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}